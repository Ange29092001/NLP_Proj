{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entraînement d'un modèle LSTM avec BERT pour la classification de texte\n",
        "\n",
        "*Membres du groupe* :\n",
        "- ADAM EZ-ZAHIR\n",
        "- ACHRAF JEMALI\n",
        "- ANGE-MARIE GOUNADON\n",
        "- SOUROU ALFRED SOUDJI\n",
        "\n",
        "\n",
        "Dans ce projet, nous nous pencherons sur le développement d'un modèle de classification de texte en utilisant une approche plus avancée. Notre choix se porte sur l'intégration du modèle BERT (Bidirectional Encoder Representations from Transformers) dans une architecture LSTM (Long Short-Term Memory).\n",
        "\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Pour notre expérimentation, nous utiliserons le jeu de données AG_NEWS, une collection de titres d'articles de nouvelles accompagnés de descriptions provenant de diverses sources d'informations. Organisé en quatre catégories principales (Monde, Sports, Business et Science/Technologie), ce jeu de données offre un cadre propice à la classification de texte, permettant à notre modèle de prédire automatiquement la catégorie d'une nouvelle donnée.\n",
        "\n",
        "**Préparation de l'environnement**\n",
        "\n",
        "Nous débuterons par la configuration de notre environnement de développement en important les bibliothèques nécessaires, notamment PyTorch pour la modélisation, Transformers pour l'intégration de BERT, et d'autres modules essentiels.\n",
        "\n",
        "**Chargement des données**\n",
        "\n",
        "Le jeu de données AG_NEWS sera chargé en utilisant les utilitaires de torchtext, simplifiant le processus de téléchargement et de prétraitement des données textuelles.\n",
        "\n",
        "**Prétraitement du texte**\n",
        "\n",
        "Nous aborderons les étapes de tokenisation et de construction de vocabulaire, tout en adaptant ces processus à l'intégration de BERT pour assurer une représentation adéquate des données.\n",
        "\n",
        "**Construction du modèle LSTM avec BERT**\n",
        "\n",
        "Définir la classe du modèle LSTMClassifier sera notre prochaine étape. Cette classe incorporera l'architecture LSTM, tirant parti des fonctionnalités de BERT pour une compréhension contextuelle plus profonde du texte.\n",
        "\n",
        "**Entraînement du modèle**\n",
        "\n",
        "Nous procéderons à l'entraînement du modèle en utilisant la technique de rétropropagation à travers le temps (BPTT). Nous ajusterons la fonction de perte et les paramètres d'optimisation pour garantir des performances optimales.\n",
        "\n",
        "**Évaluation du modèle**\n",
        "\n",
        "Enfin, nous évaluerons les performances du modèle en calculant la précision sur un ensemble de test. Cela nous permettra d'apprécier la capacité prédictive du modèle dans des conditions similaires à celles d'une application réelle.\n",
        "\n",
        "Notre approche combinant LSTM et BERT vise à exploiter les avantages de chacun de ces modèles pour obtenir des résultats plus performants dans la classification de texte.\n",
        "\n"
      ],
      "metadata": {
        "id": "FjnAe7rjL17h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker>=2.0.0"
      ],
      "metadata": {
        "id": "oqPiEfQWQDAY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation des modules nécessaires\n",
        "from torchtext.datasets import AG_NEWS\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.optim as optim\n",
        "\n",
        "# Crée un itérateur pour le jeu de données AG News avec l'ensemble d'entraînement\n",
        "train_iter = iter(AG_NEWS(split='train'))\n",
        "\n",
        "# Récupère le premier exemple du jeu de données\n",
        "first_example = next(train_iter)\n",
        "\n",
        "# Affiche le premier exemple\n",
        "print(first_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0UtB11nPnyF",
        "outputId": "d6565c16-72ba-4362-f8c6-cc0deea7af86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code suivant définit une classe `BertLSTMClassifier` qui combine un modèle BERT pré-entraîné avec une couche LSTM pour la classification de texte. L'objectif est de capturer des représentations riches du texte avec BERT et de les traiter avec une couche LSTM pour effectuer la classification. Les paramètres de BERT sont figés pour conserver les poids pré-entraînés. La méthode `forward` spécifie le flux de données à travers le modèle lors de la propagation avant. Le modèle est conçu pour la tâche de classification de textes, en utilisant le jeu de données AG News comme exemple."
      ],
      "metadata": {
        "id": "QGPqX-lzVLMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition d'un classificateur LSTM avec BERT\n",
        "class BertLSTMClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_classes, bert_model_name='bert-base-uncased'):\n",
        "        super(BertLSTMClassifier, self).__init__()\n",
        "\n",
        "        # Initialisation du modèle BERT pré-entraîné\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        # Couche LSTM prenant la sortie de BERT\n",
        "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Couche entièrement connectée pour la classification\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        # Désactivation de l'apprentissage pour les paramètres de BERT\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Exécution de BERT sur les données d'entrée (avec la désactivation des gradients)\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Passe la sortie de BERT à travers la couche LSTM\n",
        "        lstm_out, _ = self.lstm(bert_output.last_hidden_state)\n",
        "\n",
        "\n",
        "        # Sélectionne la dernière sortie de la séquence LSTM\n",
        "        final_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Passe la sortie finale à travers la couche entièrement connectée pour la classification\n",
        "        return self.fc(final_out)\n",
        "\n",
        "# Utilisation de l'itérateur pour obtenir un exemple du jeu de données AG News\n",
        "next(iter(train_iter))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMSolmO-QI1I",
        "outputId": "622727d6-1786-4dd1-d1f6-acdd4fb21bab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "le code suivant que nous avons rédigé a pour objectif l'utilisation du modèle BERT pour la classification de texte sur le jeu de données AG News. Il configure l'environnement GPU si disponible, télécharge les données d'entraînement et de test AG News, utilise le tokenizer BERT pour traiter les textes, et crée des DataLoader pour l'entraînement et le test avec un batch_size de 8. L'objectif global est de préparer les données dans un format compatible avec BERT et de créer des DataLoader pour faciliter l'entraînement d'un modèle de classification de texte avec BERT"
      ],
      "metadata": {
        "id": "Tpfw0sDpWGiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifie la disponibilité de GPU et utilise CUDA s'il est disponible, sinon utilise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Télécharge les jeux de données d'entraînement et de test AG News\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "\n",
        "# Initialise le tokenizer BERT avec le modèle 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fonction pour regrouper un batch de données pour l'entraînement avec BERT\n",
        "def collate_batch_bert(batch):\n",
        "    # Récupère les textes et les étiquettes du batch\n",
        "    texts = [x[1] for x in batch]\n",
        "    labels = torch.tensor([x[0]-1 for x in batch], dtype=torch.long)  # Ajuste les étiquettes de 1 à n_classes\n",
        "\n",
        "    # Tokenisation des textes avec BERT, avec gestion du padding et troncature\n",
        "    encoding = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    input_ids = encoding['input_ids']  # Identifiants d'entrée BERT\n",
        "    attention_mask = encoding['attention_mask']  # Masque d'attention pour les séquences\n",
        "\n",
        "    # Retourne les étiquettes et les données tokenisées\n",
        "    return labels.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "# Crée des DataLoader pour l'entraînement et le test avec le batch_size de 8\n",
        "train_loader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch_bert)\n",
        "test_loader = DataLoader(test_iter, batch_size=8, shuffle=False, collate_fn=collate_batch_bert)\n"
      ],
      "metadata": {
        "id": "NFqAk3cbWBik"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons configuré le modèle pour la classification de texte en utilisant la classe BertLSTMClassifier. Nous avons définit les dimensions pour l'embedding, la couche cachée, et le nombre de classes. Ensuite, Nous avons instancié un objet de cette classe avec les dimensions spécifiées. Enfin, Nous avons définit une fonction de perte (CrossEntropyLoss) adaptée à la classification multiclasse, et configuré un optimiseur Adam avec un taux d'apprentissage de 0.001 pour l'entraînement du modèle. L'objectif global est de préparer le modèle pour l'entraînement en spécifiant ses caractéristiques et les paramètres d'entraînement.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x3sh3L4YW83j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition des dimensions pour l'embedding, la couche cachée, et le nombre de classes\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_classes = 4\n",
        "\n",
        "# Instanciation d'un objet de la classe BertLSTMClassifier avec les dimensions spécifiées\n",
        "model = BertLSTMClassifier(hidden_dim, num_classes).to(device)\n",
        "\n",
        "# Définition de la fonction de perte et de l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()  # Fonction de perte pour la classification multiclasse\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimiseur Adam avec un taux d'apprentissage de 0.001"
      ],
      "metadata": {
        "id": "Ay_TirjTW7ua"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code implémente une boucle d'entraînement sur plusieurs époques pour un modèle de classification de texte utilisant BERT et LSTM. Il effectue la propagation avant, calcule la perte, effectue la rétropropagation, et met à jour les poids du modèle à chaque itération sur les batches d'entraînement. La précision d'entraînement et de test est surveillée et affichée après chaque époque. L'objectif global est d'entraîner le modèle et d'évaluer ses performances sur les données de test."
      ],
      "metadata": {
        "id": "ItljKX-Wche2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Boucle d'entraînement sur 5 époques\n",
        "for epoch in range(5):\n",
        "    model.train()  # Met le modèle en mode d'entraînement\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    correct_preds = 0  # Nombre de prédictions correctes\n",
        "    total_preds = 0  # Nombre total de prédictions\n",
        "\n",
        "    # Barre de progression pour l'époque en cours\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
        "\n",
        "    model.train()  # Met à nouveau le modèle en mode d'entraînement (au cas où)\n",
        "\n",
        "    # Boucle sur les batches d'entraînement\n",
        "    for labels, input_ids, attention_mask in progress_bar:\n",
        "        num_batches += 1\n",
        "        labels = labels.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Réinitialise les gradients\n",
        "        output = model(input_ids, attention_mask)  # Propagation avant\n",
        "\n",
        "        loss = criterion(output, labels)  # Calcul de la perte\n",
        "        loss.backward()  # Rétropropagation\n",
        "        optimizer.step()  # Mise à jour des poids du modèle\n",
        "\n",
        "        total_loss += loss.item()  # Accumulation de la perte\n",
        "\n",
        "        _, predicted_labels = torch.max(output, 1)  # Sélection des prédictions\n",
        "\n",
        "        correct_preds += (predicted_labels == labels).sum().item()  # Calcul des prédictions correctes\n",
        "        total_preds += labels.size(0)  # Accumulation du nombre total de prédictions\n",
        "\n",
        "        # Mise à jour de la barre de progression\n",
        "        progress_bar.set_postfix({\n",
        "            'Train Loss': loss.item(),\n",
        "            'Train Acc': 100. * correct_preds / total_preds\n",
        "        })\n",
        "\n",
        "    # Calcul de la perte moyenne et de la précision d'entraînement\n",
        "    train_loss = total_loss / num_batches\n",
        "    train_accuracy = 100. * correct_preds / total_preds\n",
        "\n",
        "    # Mise en mode évaluation pour l'évaluation sur les données de test\n",
        "    model.eval()\n",
        "\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    # Boucle sur les batches de test\n",
        "    for labels, input_ids, attention_mask in test_loader:\n",
        "        labels = labels.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        output = model(input_ids, attention_mask)  # Propagation avant sur les données de test\n",
        "\n",
        "        _, predicted_labels = torch.max(output, 1)  # Sélection des prédictions\n",
        "\n",
        "        correct_preds += (predicted_labels == labels).sum().item()  # Calcul des prédictions correctes\n",
        "        total_preds += labels.size(0)  # Accumulation du nombre total de prédictions\n",
        "\n",
        "    # Calcul de la précision sur les données de test\n",
        "    test_accuracy = 100 * correct_preds / total_preds\n",
        "\n",
        "    # Affichage des résultats de l'époque\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}, Test Accuracy: {test_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFLf-LMycePO",
        "outputId": "2fc96b28-24e7-40ae-c880-f339edd35da0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 15000it [11:58, 20.88it/s, Train Loss=0.00434, Train Acc=90.4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.2795, Train Accuracy: 90.42, Test Accuracy: 91.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 15000it [11:55, 20.96it/s, Train Loss=0.00264, Train Acc=92.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Train Loss: 0.2164, Train Accuracy: 92.41, Test Accuracy: 92.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 15000it [11:56, 20.93it/s, Train Loss=0.00329, Train Acc=93.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Train Loss: 0.1935, Train Accuracy: 93.17, Test Accuracy: 92.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 15000it [11:56, 20.94it/s, Train Loss=0.00134, Train Acc=93.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Train Loss: 0.1798, Train Accuracy: 93.59, Test Accuracy: 92.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 15000it [11:56, 20.94it/s, Train Loss=0.000518, Train Acc=94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Train Loss: 0.1702, Train Accuracy: 94.01, Test Accuracy: 92.61%\n"
          ]
        }
      ]
    }
  ]
}